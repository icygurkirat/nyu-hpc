1. MPI-parallel jacobi with blocking send/recv: jacobi.cpp
   MPI-parallel jacobi with non-blocking send/recv: jacobinb.cpp

   Command line arguments: Nl, number_of_iterations.

   To get the timing values, I have removed the extra printing statements in the iteration loop.
   I also compare the result of each process to a sequential implementation for validating that the parallel result is correct. That time is not included in the simulation time.

   Weak scaling simulations are run on Intel(R) IvyBridge @ 3.00GHz


   Simulation 1.1: Weak scaling with Nl = 100, iterations = 5000. All tasks belong to separate nodes i.e. Tasks per node = 1
   Tasks            N=sqrt(Tasks)*Nl            Jacobi          Jacobi-NonBlocking
   1                100                         0.385691s       0.385335s
   4                200                         0.645569s       0.473397s
   16               400                         0.817367s       0.576030s

   Simulation 1.2: Weak scaling with Nl = 100, iterations = 5000. Tasks per node = 4, and hence there is less communication across network required
   Tasks            N=sqrt(Tasks)*Nl            Jacobi          Jacobi-NonBlocking
   4                200                         0.438185s       0.409364s
   16               400                         0.725007s       0.548178s
   64               800                         2.286943s       3.211810s

   Simulation 1.3: Nl = 100, iterations = 5000. Tasks per node = 16
   Tasks            N=sqrt(Tasks)*Nl            Jacobi          Jacobi-NonBlocking
   64               800                         0.888407s       0.799347s          

   Whenever N is doubled, the problem size increases by 4 times. From the data above, we can see that parallel implementation provides some speedup because 
   on doubling of N, the time for execution is not increased 4 times. But for the algorithm to be perfectly weakly scalable, this time should remain same.
   Hence, the non-blocking jacobi has better weak scalability as compared to blocking jacobi because time values remain fairly similar.

   Time also increases significantly when world size >= 64 tasks.




   The strong scaling simulations are run on Intel(R) Broadwell @ 2.60GHz
   Simulation 2: Strong scaling with N = 20000, iterations = 100.
   Tasks            TasksPerNode                Nl                   Jacobi               Jacobi-NonBlocking         Speedup(of blocking jacobi)
   1                1                           20000                127.174853s          122.001996s                - 
   4                4                           10000                26.390795s           26.030267s                 4.81               
   16               8                           5000                 9.983777s            10.071566s                 12.74             
   64               16                          2500                 3.096899s            3.172177s                  41.06  
   256              16                          1250                 

   For large N values, non-blocking jacobi does not provide a very big advantage over blocking jacobi.





2. Sample sort. File = ssort.cpp
   Accepts a command line argument N, for number of elements per partition. Default value = 100.
   I have followed the exact steps mentioned in the comments of original file.
   As an extra check I am printing the range of data, and size of each subarray to verify that subarray of partition i < subarray of partition i+1.


   All the simulations are run on Intel(R) Broadwell @ 2.60GHz
   
   Simulation 1: N=10000
   Tasks            nodes           tasksPerNode            Time
   1                1               1                       0.000587s
   2                1               2                       0.001689s
   4                1               4                       0.001202s
   8                1               8                       0.001068s
   16               1               16                      0.001283s
   32               2               16                      0.009226s
   64               4               16                      0.031584s

   Simulation 2: N=100000
   Tasks            nodes           tasksPerNode            Time
   1                1               1                       0.007454s
   2                1               2                       0.014520s
   4                1               4                       0.012237s
   8                1               8                       0.011387s
   16               1               16                      0.012273s
   32               2               16                      0.024199s
   64               4               16                      0.044038s

   Simulation 3: N=1000000
   Tasks            nodes           tasksPerNode            Time
   1                1               1                       0.082439s
   2                1               2                       0.179690s
   4                1               4                       0.134323s
   8                1               8                       0.122005s
   16               1               16                      0.115205s
   32               2               16                      0.171403s            
   64               4               16                      0.223539s

   There is a very long waiting period when you need more than 16 nodes. Hence I couldn't collect more data points.
   From this data it is clear that there is a sudden jump in time when the tasks go beyond one node, and so the communication cost becomes high.
